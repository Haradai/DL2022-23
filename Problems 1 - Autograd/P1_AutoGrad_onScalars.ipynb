{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating our own Auto Differentiation (AutoGrad) framework\n",
    "\n",
    "In this practical exercise we will build our own, very simple, Auto-differentiation (or AutoGrad) framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding the framework\n",
    "\n",
    "### Step 1: Define a class for our variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key idea is that we will define our own class of `Variable` which is basically the same as a scalar (a number). So our class is created by passing it a `value`, and it stores this value internally.\n",
    "\n",
    "But apart from being a placeholder for a number, we also want to keep track of the way every variable was created.\n",
    "\n",
    "For example, if a variable $c$ is the result of the addition of two variables $a$ and $b$: $c = a + b$, then we would say that $a$ and $b$ are \"parent\" variables of $c$, and $c$ is their \"child\". The way $c$ was created was by adding these two parent variables together.\n",
    "\n",
    "So apart from the value of the variable, we will also have to keep track of the parents, and on how each of them \"contributes\" to the creation of the variable - this is described by the local derivative associated with each of the parents, that tells us how a change in each of the parent variables translates into a change in the child variable.\n",
    "\n",
    "This is important in order to implement our backwards pass. Each parent defines a \"route\" through which the gradients coming into this variable will have to flow through. So we will define a list of `gradRoutes` that will contain the list of parent variables and their corresponding local derivatives. A variable created directly (not resulting by any operation over existing variables) will have an empty `gradRoutes`.\n",
    "\n",
    "Finally, we want each variable of ours to keep also the value of gradient of the quantity we are interested in with respect to that variable. We will create a placeholder for that as well, called `grad`. As seen before, this will accummulate the gradients that are backpropagated from the children of this variable when we implement the backpropagation algorithm. So we will initialise it to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Variable: #Simple variable. These are the leafs of our tree, they can request to have a gradient calculated, or not\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "        self.gradRoutes = [] # A variable by default has no grad Routes (was not created by anything, just defined)\n",
    "        self.grad = 0.0        \n",
    "        \n",
    "    def __str__(self):\n",
    "        return 'Value: {self.value}'.format(self=self)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from the `__init__()` function which stores the value passed to our class and initialises the `gradRoutes` and `grad` member variables, we have also overloaded the function that python uses to convert a class into a string representation: `__str__()`. This will allow us to print our class.\n",
    "\n",
    "We cannot do much yet with this class, apart from storing values into our variables and printing them out. Let's try this out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value: 4.3\n",
      "Value: 5.2\n"
     ]
    }
   ],
   "source": [
    "a = Variable(4.3)\n",
    "b = Variable(5.2)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define operations over our variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step would be to implement operations on our variables. Let's first define the operations for addition and multiplication.\n",
    "\n",
    "These will be functions that take two variables as input and produce a new (child) variable with a value equal to the sum or the product of the two inputs. Apart from the forward pass though, we should keep track of how this new variable was created: the two parent variables, and their corresponding local derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vAdd(A: Variable, B: Variable): # Addition\n",
    "    result = Variable(A.value + B.value) # Create a new Variable to store the result, and pass it the value = a + b\n",
    "          \n",
    "    #keep track of the parent variables, and of the local derivative associated with each one\n",
    "    result.gradRoutes.append((A, 1)) # dresult / dA = 1\n",
    "    result.gradRoutes.append((B, 1)) # dresult / dB = 1\n",
    "    \n",
    "    return result\n",
    "    \n",
    "def vMul(A: Variable, B: Variable): # Addition\n",
    "    result = Variable(A.value * B.value) # Create a new Variable to store the result, and pass it the value = a * b\n",
    "          \n",
    "    #keep track of the parent variables, and of the local derivative associated with each one\n",
    "    result.gradRoutes.append((A, B.value)) # dresult / dA = B\n",
    "    result.gradRoutes.append((B, A.value)) # dresult / dB = A\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for example to calculate $d = (a + b) * c$ we first need to calculate $(a + b)$ and then mutiply the result with $c$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value: 20\n"
     ]
    }
   ],
   "source": [
    "a = Variable(2) # a = 2\n",
    "b = Variable(3) # b = 3\n",
    "c = Variable(4) # c = 4\n",
    "\n",
    "#d = (a + b) * c = 20\n",
    "d = vMul(vAdd(a, b), c)\n",
    "\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Implement the backpropagation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to implement the backpropagation function. This starts with a child variable, and backpropagates gradients through the routes defined recursively. It uses the two rules that we saw before:\n",
    "\n",
    "- Accumulate the incoming gradients from the different grad routes that lead to a node (a variable). Each of the incoming gradients describe a different way in which the node affects the quantity of interest, so this sum will be the final gradient for the node\n",
    "- Multiply every incoming gradient with each of the local derivatives corresponding to parent variables (this would be the application of the chain rule), and continue the backpropagation through the corresponding route (for each of the parent variables)\n",
    "\n",
    "We update the `Variable` class accordingly. We also update the `__str___()` function to include also gradient information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Variable: #Simple variable. These are the leafs of our tree, they can request to have a gradient calculated, or not\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "        self.gradRoutes = [] # A variable by default has no grad Routes (was not created by anything, just defined)\n",
    "        self.grad = 0.0\n",
    "    \n",
    "    def backProp(self, route_val = 1.0):\n",
    "        # Add together the incoming gradients from the different routes that lead to a node - this will be the final gradient for the node\n",
    "        self.grad += route_val\n",
    "                \n",
    "        # For every parent variable and corresponding local derivative value that we have in the gradRoutes, continue with the gradient calculation\n",
    "        for variable, local_derivative_value in self.gradRoutes:\n",
    "            # Multiply the incoming gradient with the local derivatives corresponding the parent variable, and continue the backpropagation\n",
    "            variable.backProp(local_derivative_value * route_val)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'Value: {self.value}, Gradient: {self.grad}'.format(self=self)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>**Question:** Why did we set the default value of route_val equal to 1.0?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Because that is the original node from which we are starting to calculate. The derivative of that node by itself is one.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should be all. If we want to calculate the derivative of the result with respect to any of the variables that participated in the calculation, we just need to call backprop on the result, and then read the derivatives out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result = 20\n",
      "The derivative of the result with respect to a is: 4.0\n",
      "The derivative of the result with respect to b is: 4.0\n",
      "The derivative of the result with respect to c is: 5.0\n"
     ]
    }
   ],
   "source": [
    "a = Variable(2)           # a = 2\n",
    "b = Variable(3)           # b = 3\n",
    "c = Variable(4)           # c = 4\n",
    "res = vMul(vAdd(a, b), c) # res = (a + b) * c = 20\n",
    "\n",
    "print(\"Result =\", res.value)\n",
    "\n",
    "# Call backprop on the result\n",
    "res.backProp()\n",
    "\n",
    "# Now all variables should contain in their \"grad\" the derivative d(res) / d(variable)\n",
    "print(\"The derivative of the result with respect to a is:\", a.grad)\n",
    "print(\"The derivative of the result with respect to b is:\", b.grad)\n",
    "print(\"The derivative of the result with respect to c is:\", c.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following example, variable $a$ affects the result through two different routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result = 28\n",
      "The derivative of the result with respect to a is: 11.0\n",
      "The derivative of the result with respect to b is: 4.0\n",
      "The derivative of the result with respect to c is: 4.0\n"
     ]
    }
   ],
   "source": [
    "a = Variable(4)  # a = 4\n",
    "b = Variable(3)  # b = 3\n",
    "c = vAdd(a, b)   # c = 4 + 3\n",
    "res = vMul(a, c) # res = a * c = 28\n",
    "\n",
    "print(\"Result =\", res.value)\n",
    "\n",
    "# Call backprop on the result\n",
    "res.backProp()\n",
    "\n",
    "# Now all variables should contain in their \"grad\" the derivative d(res) / d(variable)\n",
    "print(\"The derivative of the result with respect to a is:\", a.grad)\n",
    "print(\"The derivative of the result with respect to b is:\", b.grad)\n",
    "# Also for intermediate results\n",
    "print(\"The derivative of the result with respect to c is:\", c.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>**Question:** Can you now use this setup to calculate the derivative of $c$ with respect to $a$ and $b$?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The derivative of c with respect to a is: 1.0\n",
      "The derivative of c with respect to b is: 1.0\n"
     ]
    }
   ],
   "source": [
    "#first reset the gradients\n",
    "a.grad = 0\n",
    "b.grad = 0\n",
    "c.grad = 0\n",
    "c.backProp()\n",
    "print(\"The derivative of c with respect to a is:\", a.grad)\n",
    "print(\"The derivative of c with respect to b is:\", b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final touches\n",
    "\n",
    "If you understood how this works up to here, then you should be already good to go. But since we want to use our auto grad to do some practical work, we will continue working on it a bit, to make it a bit more usable and complete it with more operations. Many of the subsequent steps are quite \"engineering\" in nature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving usability: overloading operators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course this is still highly incomplete, very inefficient and not very usable. Lets first improve a usability issue. Instead of having to call different functions for the operations like `res = vMul(a, c)`, we would like to be able to directly write them down like `res = a * b`. To achieve this, we should overload [Python's special functions for operator overloading](https://docs.python.org/3/reference/datamodel.html#emulating-numeric-types).\n",
    "\n",
    "Here's how to do this for the addition and multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Variable: #Simple variable. These are the leafs of our tree, they can request to have a gradient calculated, or not\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "        self.gradRoutes = [] # A variable by default has no grad Routes (was not created by anything, just defined)\n",
    "        self.grad = 0.0\n",
    "    \n",
    "    def backProp(self, route_val = 1.0):\n",
    "        # Add together the incoming gradients from the different routes that lead to a node - this will be the final gradient for the node\n",
    "        self.grad += route_val\n",
    "                \n",
    "        # For every parent variable and corresponding local derivative value that we have in the gradRoutes, continue with the gradient calculation\n",
    "        for variable, local_derivative_value in self.gradRoutes:\n",
    "            # Multiply the incoming gradient with the local derivatives corresponding the parent variable, and continue the backpropagation\n",
    "            variable.backProp(local_derivative_value * route_val)\n",
    "            \n",
    "    def __add__(self, b):\n",
    "        return vAdd(self, b)\n",
    "        \n",
    "    def __mul__(self, b):\n",
    "        return vMul(self, b)            \n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'Value: {self.value}, Gradient: {self.grad}'.format(self=self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result = 28\n",
      "The derivative of the result with respect to a is: 11.0\n",
      "The derivative of the result with respect to b is: 4.0\n",
      "The derivative of the result with respect to c is: 4.0\n"
     ]
    }
   ],
   "source": [
    "a = Variable(4)  # a = 4\n",
    "b = Variable(3)  # b = 3\n",
    "c = a + b        # c = 4 + 3\n",
    "res = a * c      # res = a * c = 28\n",
    "\n",
    "print(\"Result =\", res.value)\n",
    "\n",
    "# Call backprop on the result\n",
    "res.backProp()\n",
    "\n",
    "# Now all variables should contain in their \"grad\" the derivative d(res) / d(variable)\n",
    "print(\"The derivative of the result with respect to a is:\", a.grad)\n",
    "print(\"The derivative of the result with respect to b is:\", b.grad)\n",
    "# Also for intermediate results\n",
    "print(\"The derivative of the result with respect to c is:\", b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zeroing gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A last thing to note is that once we call `backProp`, our gradients are calculated and our variables are now \"dirty\" in the sense that if we call backprop again, the new result will be added to the previous one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The derivative of the result with respect to a is: 11.0\n",
      "The derivative of the result with respect to b is: 4.0\n",
      "Second time\n",
      "The derivative of the result with respect to a is: 22.0\n",
      "The derivative of the result with respect to b is: 8.0\n"
     ]
    }
   ],
   "source": [
    "a = Variable(4)   # a = 4\n",
    "b = Variable(3)   # b = 3\n",
    "res = (a + b) * a # res = a * c = 28\n",
    "\n",
    "# Call backprop on the result\n",
    "res.backProp()\n",
    "print(\"The derivative of the result with respect to a is:\", a.grad)\n",
    "print(\"The derivative of the result with respect to b is:\", b.grad)\n",
    "\n",
    "# Call backprop on the result once more\n",
    "print(\"Second time\")\n",
    "res.backProp()\n",
    "print(\"The derivative of the result with respect to a is:\", a.grad)\n",
    "print(\"The derivative of the result with respect to b is:\", b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will actually turn out to be quite useful, e.g. when we want to accumulate weight gradients over different samples in our learning loops (see next week's notebook), but we need a way to control it.\n",
    "\n",
    "To avoid this, we should reset the gradients to zero before we call `backProp` again. We can do it one by one for every variable, but we will also implement a function that does this recursively from the child node we backProped, all the way to the parents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Variable: #Simple variable. These are the leafs of our tree, they can request to have a gradient calculated, or not\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "        self.gradRoutes = [] # A variable by default has no grad Routes (was not created by anything, just defined)\n",
    "        self.grad = 0.0\n",
    "    \n",
    "    def backProp(self, route_val = 1.0):\n",
    "        # Add together the incoming gradients from the different routes that lead to a node - this will be the final gradient for the node\n",
    "        self.grad += route_val\n",
    "                \n",
    "        # For every parent variable and corresponding local derivative value that we have in the gradRoutes, continue with the gradient calculation\n",
    "        for variable, local_derivative_value in self.gradRoutes:\n",
    "            # Multiply the incoming gradient with the local derivatives corresponding the parent variable, and continue the backpropagation\n",
    "            variable.backProp(local_derivative_value * route_val)\n",
    "\n",
    "    def zeroGrad(self):\n",
    "        self.grad = 0.0\n",
    "        \n",
    "    def zeroGradsRecursively(self):\n",
    "        self.zeroGrad()\n",
    "        for variable, _ in self.gradRoutes:\n",
    "            variable.zeroGradsRecursively()\n",
    "            \n",
    "    def __add__(self, b):\n",
    "        return vAdd(self, b)\n",
    "        \n",
    "    def __mul__(self, b):\n",
    "        return vMul(self, b)           \n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'Value: {self.value}, Gradient: {self.grad}'.format(self=self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The derivative of the result with respect to a is: 11.0\n",
      "The derivative of the result with respect to b is: 4.0\n",
      "Second time\n",
      "The derivative of the result with respect to a is: 11.0\n",
      "The derivative of the result with respect to b is: 4.0\n"
     ]
    }
   ],
   "source": [
    "a = Variable(4)   # a = 4\n",
    "b = Variable(3)   # b = 3\n",
    "res = (a + b) * a # res = a * c = 28\n",
    "\n",
    "# Call backprop on the result\n",
    "res.backProp()\n",
    "print(\"The derivative of the result with respect to a is:\", a.grad)\n",
    "print(\"The derivative of the result with respect to b is:\", b.grad)\n",
    "\n",
    "# Zero gradients\n",
    "res.zeroGradsRecursively()\n",
    "\n",
    "# Call backprop on the result once more\n",
    "print(\"Second time\")\n",
    "res.backProp()\n",
    "print(\"The derivative of the result with respect to a is:\", a.grad)\n",
    "print(\"The derivative of the result with respect to b is:\", b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Improvements - Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are number of ways we can improve our simple network. The most important is probably being able to work with vectors and matrices - we will not implement this ourselves though, next week we will see a framework that does this. For the time being, we will focus on other, simpler improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### <font color=blue>Excercise 1 (Easy):</font>\n",
    "    \n",
    "<font color=blue>We usually do not require gradients for all our variables. If we could indicate which variables require gradients, then we could keep track of the routes that lead to these variables only and drop all the rest. This would be a huge improvement in resources and speed (number of calculations). Can you add this functionality?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We keep track of each variables and the generated by the operators by having a depth level.\n",
    "\n",
    "We'll also have a function that prints them all out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Variable: #Simple variable. These are the leafs of our tree, they can request to have a gradient calculated, or not\n",
    "    def __init__(self, value,level=0):\n",
    "        self.value = value\n",
    "        self.gradRoutes = [] # A variable by default has no grad Routes (was not created by anything, just defined)\n",
    "        self.grad = 0.0\n",
    "        self.lev = level\n",
    "    \n",
    "    def backProp(self, route_val = 1.0):\n",
    "        # Add together the incoming gradients from the different routes that lead to a node - this will be the final gradient for the node\n",
    "        self.grad += route_val\n",
    "                \n",
    "        # For every parent variable and corresponding local derivative value that we have in the gradRoutes, continue with the gradient calculation\n",
    "        for variable, local_derivative_value in self.gradRoutes:\n",
    "            # Multiply the incoming gradient with the local derivatives corresponding the parent variable, and continue the backpropagation\n",
    "            variable.backProp(local_derivative_value * route_val)\n",
    "\n",
    "    def print_nodes(self):\n",
    "        print(self)\n",
    "        for var, _ in self.gradRoutes:\n",
    "            var.print_nodes()\n",
    "        \n",
    "    def target_backprop(self,vars:list[Variable],route_val = 1.0)->float:\n",
    "        # Add together the incoming gradients from the different routes that lead to a node - this will be the final gradient for the node\n",
    "        self.grad += route_val\n",
    "        if self in vars:\n",
    "            vars.pop(self)\n",
    "        if(len(vars) == 0):\n",
    "            return None\n",
    "        # For every parent variable and corresponding local derivative value that we have in the gradRoutes, continue with the gradient calculation\n",
    "        for variable, local_derivative_value in self.gradRoutes:\n",
    "            # Multiply the incoming gradient with the local derivatives corresponding the parent variable, and continue the backpropagation\n",
    "            variable.backProp(local_derivative_value * route_val)\n",
    "            \n",
    "    def zeroGrad(self):\n",
    "        self.grad = 0.0\n",
    "        \n",
    "    def zeroGradsRecursively(self):\n",
    "        self.zeroGrad()\n",
    "        for variable, _ in self.gradRoutes:\n",
    "            variable.zeroGradsRecursively()\n",
    "            \n",
    "    def __add__(self, b):\n",
    "        return vAdd(self, b)\n",
    "        \n",
    "    def __mul__(self, b):\n",
    "        return vMul(self, b)           \n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'level:{self.lev}, value: {self.value}, Gradient: {self.grad}'.format(self=self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level:0, value: 28, Gradient: 0.0\n",
      "level:0, value: 7, Gradient: 0.0\n",
      "level:0, value: 4, Gradient: 0.0\n",
      "level:0, value: 3, Gradient: 0.0\n",
      "level:0, value: 4, Gradient: 0.0\n"
     ]
    }
   ],
   "source": [
    "a = Variable(4)   # a = 4\n",
    "b = Variable(3)   # b = 3\n",
    "res = (a + b) * a # res = a * c = 28\n",
    "res.print_nodes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have the levels and index properly assigned we have to modify the operator functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vAdd(A: Variable, B: Variable): # Addition\n",
    "    result = Variable(A.value + B.value, level=A.lev+1) # Create a new Variable to store the result, and pass it the value = a + b\n",
    "          \n",
    "    #keep track of the parent variables, and of the local derivative associated with each one\n",
    "    result.gradRoutes.append((A, 1)) # dresult / dA = 1\n",
    "    result.gradRoutes.append((B, 1)) # dresult / dB = 1\n",
    "    \n",
    "    return result\n",
    "    \n",
    "def vMul(A: Variable, B: Variable): # Addition\n",
    "    result = Variable(A.value * B.value, level=A.lev+1) # Create a new Variable to store the result, and pass it the value = a * b\n",
    "          \n",
    "    #keep track of the parent variables, and of the local derivative associated with each one\n",
    "    result.gradRoutes.append((A, B.value)) # dresult / dA = B\n",
    "    result.gradRoutes.append((B, A.value)) # dresult / dB = A\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level:2, value: 28, Gradient: 1.0\n",
      "level:1, value: 7, Gradient: 4.0\n",
      "level:0, value: 4, Gradient: 11.0\n",
      "level:0, value: 3, Gradient: 4.0\n",
      "level:0, value: 4, Gradient: 11.0\n"
     ]
    }
   ],
   "source": [
    "a = Variable(4)   # a = 4\n",
    "b = Variable(3)   # b = 3\n",
    "res = (a + b) * a # res = a * c = 28\n",
    "res.backProp()\n",
    "res.print_nodes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we can see that we can search for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level:0, value: 4, Gradient: 11.0\n"
     ]
    }
   ],
   "source": [
    "print(res.gradRoutes[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_print_grads(Variable):\n",
    "    print(Variable)\n",
    "    for var, _ in Variable.gradRoutes:\n",
    "        recursive_print_grads(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level:2, value: 28, Gradient: 1.0\n",
      "level:1, value: 7, Gradient: 4.0\n",
      "level:0, value: 4, Gradient: 11.0\n",
      "level:0, value: 3, Gradient: 4.0\n",
      "level:0, value: 4, Gradient: 11.0\n"
     ]
    }
   ],
   "source": [
    "recursive_print_grads(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level:2, value: 28, Gradient: 1.0\n",
      "level:1, value: 7, Gradient: 4.0\n",
      "level:0, value: 4, Gradient: 11.0\n",
      "level:0, value: 3, Gradient: 4.0\n",
      "level:0, value: 4, Gradient: 11.0\n"
     ]
    }
   ],
   "source": [
    "a = Variable(4)   # a = 4\n",
    "b = Variable(3)   # b = 3\n",
    "res = (a + b) * a # res = a * c = 28\n",
    "res.backProp()\n",
    "recursive_print_grads(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### <font color=blue>Excercise 2 (Normal):</font>\n",
    "    \n",
    "<font color=blue>We obviously need to implement more functions - implement the following functions:\n",
    "- Subtraction\n",
    "- Raising to a power\n",
    "- Division\n",
    "- Unary negation\n",
    "- The (natural) exponential function exp(x)\n",
    "- ... any other function you might want</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vAdd(A: Variable, B: Variable): # Addition\n",
    "    result = Variable(A.value + B.value) # Create a new Variable to store the result, and pass it the value = a + b\n",
    "          \n",
    "    #keep track of the parent variables, and of the local derivative associated with each one\n",
    "    result.gradRoutes.append((A, 1)) # dresult / dA = 1\n",
    "    result.gradRoutes.append((B, 1)) # dresult / dB = 1\n",
    "    \n",
    "    return result\n",
    "    \n",
    "def vMul(A: Variable, B: Variable): # Addition\n",
    "    result = Variable(A.value * B.value) # Create a new Variable to store the result, and pass it the value = a * b\n",
    "          \n",
    "    #keep track of the parent variables, and of the local derivative associated with each one\n",
    "    result.gradRoutes.append((A, B.value)) # dresult / dA = B\n",
    "    result.gradRoutes.append((B, A.value)) # dresult / dB = A\n",
    "    \n",
    "    return result\n",
    "\n",
    "def vSub(A: Variable, B: Variable): # Addition\n",
    "    result = Variable(A.value - B.value) # Create a new Variable to store the result, and pass it the value = a + b\n",
    "          \n",
    "    #keep track of the parent variables, and of the local derivative associated with each one\n",
    "    result.gradRoutes.append((A, 1)) # dresult / dA = 1\n",
    "    result.gradRoutes.append((B, -1)) # dresult / dB = 1\n",
    "    \n",
    "    return result\n",
    "    \n",
    "def vPow(A: Variable, B: Variable): # Addition\n",
    "    result = Variable(A.value**B.value) # Create a new Variable to store the result, and pass it the value = a * b\n",
    "          \n",
    "    #keep track of the parent variables, and of the local derivative associated with each one\n",
    "    result.gradRoutes.append((A, B.value*A.value**(B.value-1))) # dresult / dA = B\n",
    "    result.gradRoutes.append((B, (A.value**B.value)*math.log(B.value))) # dresult / dB = A\n",
    "    \n",
    "    return result\n",
    "\n",
    "def vDiv(A: Variable, B: Variable): # Addition\n",
    "    result = Variable(A.value / B.value) # Create a new Variable to store the result, and pass it the value = a * b\n",
    "          \n",
    "    #keep track of the parent variables, and of the local derivative associated with each one\n",
    "    result.gradRoutes.append((A, 1/B.value)) # dresult / dA = B\n",
    "    result.gradRoutes.append((B, (-1)*A.value/(B.value**2))) # dresult / dB = A\n",
    "    \n",
    "    return result\n",
    "\n",
    "def vExp(A: Variable): # Addition\n",
    "    result = Variable(math.exp(A.value)) # Create a new Variable to store the result, and pass it the value = a * b\n",
    "          \n",
    "    #keep track of the parent variables, and of the local derivative associated with each one\n",
    "    result.gradRoutes.append((A,A.value*math.exp(A.value-1))) # dresult / dA = B\n",
    "    \n",
    "    return result \n",
    "\n",
    "def vNeg(A: Variable): # Addition\n",
    "    result = Variable(A.value * -1) # Create a new Variable to store the result, and pass it the value = a * b\n",
    "          \n",
    "    #keep track of the parent variables, and of the local derivative associated with each one\n",
    "    result.gradRoutes.append((A,-1)) # dresult / dA = B\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Variable: #Simple variable. These are the leafs of our tree, they can request to have a gradient calculated, or not\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "        self.gradRoutes = [] # A variable by default has no grad Routes (was not created by anything, just defined)\n",
    "        self.grad = 0.0\n",
    "    \n",
    "    def backProp(self, route_val = 1.0):\n",
    "        # Add together the incoming gradients from the different routes that lead to a node - this will be the final gradient for the node\n",
    "        self.grad += route_val\n",
    "                \n",
    "        # For every parent variable and corresponding local derivative value that we have in the gradRoutes, continue with the gradient calculation\n",
    "        for variable, local_derivative_value in self.gradRoutes:\n",
    "            # Multiply the incoming gradient with the local derivatives corresponding the parent variable, and continue the backpropagation\n",
    "            variable.backProp(local_derivative_value * route_val)\n",
    "\n",
    "    def target_backprop(self,vars:list[Variable],route_val = 1.0)->float:\n",
    "        # Add together the incoming gradients from the different routes that lead to a node - this will be the final gradient for the node\n",
    "        self.grad += route_val\n",
    "        if self in vars:\n",
    "            vars.pop(self)\n",
    "        if(len(vars) == 0):\n",
    "            return None\n",
    "        # For every parent variable and corresponding local derivative value that we have in the gradRoutes, continue with the gradient calculation\n",
    "        for variable, local_derivative_value in self.gradRoutes:\n",
    "            # Multiply the incoming gradient with the local derivatives corresponding the parent variable, and continue the backpropagation\n",
    "            variable.backProp(local_derivative_value * route_val)\n",
    "            \n",
    "\n",
    "    def zeroGrad(self):\n",
    "        self.grad = 0.0\n",
    "        \n",
    "    def zeroGradsRecursively(self):\n",
    "        self.zeroGrad()\n",
    "        for variable, _ in self.gradRoutes:\n",
    "            variable.zeroGradsRecursively()\n",
    "            \n",
    "    def __add__(self, b):\n",
    "        return vAdd(self, b)\n",
    "\n",
    "    def __sub__(self, b):\n",
    "        return vSub(self, b)\n",
    "   \n",
    "    def __mul__(self, b):\n",
    "        return vMul(self, b) \n",
    "    \n",
    "    def __truediv__(self, b):\n",
    "        return vDiv(self, b)\n",
    "\n",
    "    def __pow__(self, b):\n",
    "        return vPow(self,b)         \n",
    "    \n",
    "    def __neg__(self):\n",
    "        return vNeg(self)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return 'Value: {self.value}, Gradient: {self.grad}'.format(self=self)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value: 1.7550546569602985, Gradient: 1.0\n",
      "Value: 0.5625, Gradient: 0.3631772961156893\n",
      "Value: 36, Gradient: 0.005674645251807645\n",
      "Value: 9, Gradient: 0.02269858100723058\n",
      "Value: 1, Gradient: 0.02269858100723058\n",
      "Value: 4, Gradient: 0.07377038827349938\n",
      "Value: 3, Gradient: -0.02269858100723058\n",
      "Value: 8, Gradient: -0.028373226259038223\n",
      "Value: 4, Gradient: 0.07377038827349938\n",
      "Value: 64, Gradient: -0.0031919879541418\n",
      "Value: 8, Gradient: -0.028373226259038223\n",
      "Value: 2, Gradient: -0.14160111685086058\n"
     ]
    }
   ],
   "source": [
    "a = Variable(4)   # a = 4\n",
    "b = Variable(3)   # b = 3\n",
    "c = Variable(8)\n",
    "e = Variable(2)\n",
    "#res = a / b\n",
    "res = vExp(((a - b + c) * a ) / (c**(e)))# res = a * c = 28\n",
    "res.backProp()\n",
    "recursive_print_grads(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### <font color=blue>Excercise 3 (Difficult):</font>\n",
    "    \n",
    "<font color=blue>Our operations currently accept only instances of our Variable class as inputs. So, if you wanted to calculate `a = b * 2` where `b` is an instance of our variable class and `2` is just a numerical constant you would get an error as our framework does not know how to multiply a `Variable` with a number. You should instead write `a = b * Variable(2)` to achieve this.</font>\n",
    "\n",
    "<font color=blue>Improve further the usability of our framework by allowing our functions to mix numbers and Variables in the same operation (look also into the overloads of the [reflected operands in python](https://docs.python.org/3/reference/datamodel.html#object.__radd__))</font>\n",
    "\n",
    "*Hint: look at the `isinstance()` function*\n",
    "\n",
    "*Hint: You might also want to look for reflected operators...*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Variable: #Simple variable. These are the leafs of our tree, they can request to have a gradient calculated, or not\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "        self.gradRoutes = [] # A variable by default has no grad Routes (was not created by anything, just defined)\n",
    "        self.grad = 0.0\n",
    "    \n",
    "    def backProp(self, route_val = 1.0):\n",
    "        # Add together the incoming gradients from the different routes that lead to a node - this will be the final gradient for the node\n",
    "        self.grad += route_val\n",
    "                \n",
    "        # For every parent variable and corresponding local derivative value that we have in the gradRoutes, continue with the gradient calculation\n",
    "        for variable, local_derivative_value in self.gradRoutes:\n",
    "            # Multiply the incoming gradient with the local derivatives corresponding the parent variable, and continue the backpropagation\n",
    "            variable.backProp(local_derivative_value * route_val)\n",
    "\n",
    "    def target_backprop(self,vars:list[Variable],route_val = 1.0)->float:\n",
    "        # Add together the incoming gradients from the different routes that lead to a node - this will be the final gradient for the node\n",
    "        self.grad += route_val\n",
    "        if self in vars:\n",
    "            vars.pop(self)\n",
    "        if(len(vars) == 0):\n",
    "            return None\n",
    "        # For every parent variable and corresponding local derivative value that we have in the gradRoutes, continue with the gradient calculation\n",
    "        for variable, local_derivative_value in self.gradRoutes:\n",
    "            # Multiply the incoming gradient with the local derivatives corresponding the parent variable, and continue the backpropagation\n",
    "            variable.backProp(local_derivative_value * route_val)\n",
    "            \n",
    "\n",
    "    def zeroGrad(self):\n",
    "        self.grad = 0.0\n",
    "        \n",
    "    def zeroGradsRecursively(self):\n",
    "        self.zeroGrad()\n",
    "        for variable, _ in self.gradRoutes:\n",
    "            variable.zeroGradsRecursively()\n",
    "            \n",
    "    def __add__(self, b): #called if left operand is of this class\n",
    "        if isinstance(b,Variable):\n",
    "            return vAdd(self, b)\n",
    "        else:\n",
    "            return vAdd(self, Variable(b))\n",
    "    \n",
    "    def __radd__(self, b): #called if right operand is of this class\n",
    "        return vAdd(self, Variable(b))\n",
    "\n",
    "    def __sub__(self, b):  #called if left operand is of this class\n",
    "        if isinstance(b,Variable):\n",
    "            return vSub(self, b)\n",
    "        else:\n",
    "            return vSub(self, Variable(b))\n",
    "    \n",
    "    def __rsub__(self, b): #called if right operand is of this class\n",
    "        return vSub(self, Variable(b))\n",
    "\n",
    "    def __mul__(self, b): \n",
    "        if isinstance(b,Variable):\n",
    "            return vMul(self, b) \n",
    "        else:\n",
    "            return vMul(self, Variable(b))\n",
    "\n",
    "    def __rmul__(self, b): #called if right operand is of this class\n",
    "        return vMul(self, Variable(b))\n",
    "\n",
    "    def __truediv__(self, b):\n",
    "        if isinstance(b,Variable):\n",
    "            return vDiv(self, b)\n",
    "        else:\n",
    "            return vDiv(self, Variable(b))\n",
    "    \n",
    "    def __rtruediv__(self, b): #called if right operand is of this class\n",
    "        return vDiv(self, Variable(b))\n",
    "\n",
    "    def __pow__(self, b):\n",
    "        if isinstance(b,Variable):\n",
    "            return vPow(self,b)         \n",
    "        else:\n",
    "            return vPow(self, Variable(b))\n",
    "    \n",
    "    def __rpow__(self, b): #called if right operand is of this class\n",
    "        return vPow(self, Variable(b))\n",
    "    \n",
    "    def __neg__(self):\n",
    "        return vNeg(self)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return 'Value: {self.value}, Gradient: {self.grad}'.format(self=self)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value: 0, Gradient: 1.0\n",
      "Value: 2, Gradient: 1.0\n",
      "Value: 5, Gradient: 1.0\n",
      "Value: 4, Gradient: 1.0\n",
      "Value: 1, Gradient: 1.0\n",
      "Value: 3, Gradient: -1.0\n",
      "Value: 2, Gradient: -1.0\n"
     ]
    }
   ],
   "source": [
    "a = Variable(4)   # a = 4\n",
    "b = Variable(3)   # b = 3\n",
    "c = Variable(8)\n",
    "e = Variable(2)\n",
    "res = 1 + a - b - 2\n",
    "res.backProp()\n",
    "recursive_print_grads(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value: 6.616261056709485e-112, Gradient: 1.0\n",
      "Value: -256.0, Gradient: -6.231005235677854e-110\n",
      "Value: -131072.0, Gradient: -1.216993210093331e-112\n",
      "Value: 262144, Gradient: 6.084966050466654e-113\n",
      "Value: 0, Gradient: 6.084966050466654e-113\n",
      "Value: 4, Gradient: 1.0\n",
      "Value: 4, Gradient: -6.084966050466654e-113\n",
      "Value: 262144, Gradient: 6.084966050466654e-113\n",
      "Value: 8, Gradient: 5.98176502625074e-108\n",
      "Value: 6, Gradient: 2.858102434261858e-107\n",
      "Value: -0.5, Gradient: -3.1902746806670613e-107\n",
      "Value: 512, Gradient: -3.115502617838927e-110\n",
      "Value: 8, Gradient: 5.98176502625074e-108\n",
      "Value: 3, Gradient: -1.7524374842037816e-107\n"
     ]
    }
   ],
   "source": [
    "res = vExp(((a - 4 + (6**c)) * -0.5 ) / (c**(3)))# res = a * c = 28\n",
    "res.backProp()\n",
    "recursive_print_grads(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### <font color=blue>Excercise 4 (Very Difficult):</font>\n",
    "    \n",
    "<font color=blue>Instead of keeping track of the local derivative value and doing the multiplication between the local derivative value and the incoming gradient explicitly, it is better to keep track of the actual function used to calculate the local derivative and combine it with the incoming one (the `route_val`). The idea is to keep a note of the recipe for calculating the local derivative value instead of the value itself.</font>\n",
    "\n",
    "<font color=blue>In our case, since we deal with scalars, this function would basically boil down to doing this multiplication with the `route_val` (applying the chain rule), but when we vectorise our inputs and start dealing with tensors instead of scalars (and `route_val` becomes a matrix), this will become a bit more complicated (a dot product). So, keeping track of the function instead of the local derivative value will allow us to easily extend this framework to this scenario.</font>\n",
    "\n",
    "<font color=blue>In addition, keeping a note of the function instead of the value, allows us to abstract away stuff. This basically means that we can build the computation graph first, with placeholder variables independently of specific input values, and then reuse it for different inputs. This is how many deep learning frameworks work.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I'll use the library functools for this\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Variable: #Simple variable. These are the leafs of our tree, they can request to have a gradient calculated, or not\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "        self.gradRoutes = [] # A variable by default has no grad Routes (was not created by anything, just defined)\n",
    "        self.grad = 0.0\n",
    "    \n",
    "    def backProp(self, route_val = 1.0):\n",
    "        # Add together the incoming gradients from the different routes that lead to a node - this will be the final gradient for the node\n",
    "        self.grad += route_val\n",
    "                \n",
    "        # For every parent variable and corresponding local derivative value that we have in the gradRoutes, continue with the gradient calculation\n",
    "        for variable, local_derivative_value in self.gradRoutes:\n",
    "            # Multiply the incoming gradient with the local derivatives corresponding the parent variable, and continue the backpropagation\n",
    "            variable.backProp(local_derivative_value * route_val)\n",
    "\n",
    "    def target_backprop(self,vars:list[Variable],route_val = 1.0)->float:\n",
    "        # Add together the incoming gradients from the different routes that lead to a node - this will be the final gradient for the node\n",
    "        self.grad += route_val\n",
    "        if self in vars:\n",
    "            vars.pop(self)\n",
    "        if(len(vars) == 0):\n",
    "            return None\n",
    "        # For every parent variable and corresponding local derivative value that we have in the gradRoutes, continue with the gradient calculation\n",
    "        for variable, local_derivative_value in self.gradRoutes:\n",
    "            # Multiply the incoming gradient with the local derivatives corresponding the parent variable, and continue the backpropagation\n",
    "            variable.backProp(local_derivative_value * route_val)\n",
    "            \n",
    "    def zeroGrad(self):\n",
    "        self.grad = 0.0\n",
    "        \n",
    "    def zeroGradsRecursively(self):\n",
    "        self.zeroGrad()\n",
    "        for variable, _ in self.gradRoutes:\n",
    "            variable.zeroGradsRecursively()\n",
    "            \n",
    "    def __add__(self, b): #called if left operand is of this class\n",
    "        if isinstance(b,Variable):\n",
    "            return partial(vAdd,(self, b))\n",
    "        else:\n",
    "            return partial(vAdd,(self, Variable(b)))\n",
    "    \n",
    "    def __radd__(self, b): #called if right operand is of this class\n",
    "        return partial(vAdd,(self, Variable(b)))\n",
    "\n",
    "    def __sub__(self, b):  #called if left operand is of this class\n",
    "        if isinstance(b,Variable):\n",
    "            return partial(vSub,(self, b))\n",
    "        else:\n",
    "            return partial(vSub,(self, Variable(b)))\n",
    "    \n",
    "    def __rsub__(self, b): #called if right operand is of this class\n",
    "        return partial(vSub,(self, Variable(b)))\n",
    "\n",
    "    def __mul__(self, b): \n",
    "        if isinstance(b,Variable):\n",
    "            return partial(vMul,(self, b))\n",
    "        else:\n",
    "            return partial(vMul,(self, Variable(b)))\n",
    "\n",
    "    def __rmul__(self, b): #called if right operand is of this class\n",
    "        return partial(vMul,(self, Variable(b)))\n",
    "\n",
    "    def __truediv__(self, b):\n",
    "        if isinstance(b,Variable):\n",
    "            return partial((vDiv,(self, b)))\n",
    "        else:\n",
    "            return partial(vDiv,(self, Variable(b)))\n",
    "    \n",
    "    def __rtruediv__(self, b): #called if right operand is of this class\n",
    "        return partial(vDiv,(self, Variable(b)))\n",
    "\n",
    "    def __pow__(self, b):\n",
    "        if isinstance(b,Variable):\n",
    "            return partial(vPow,(self,b))     \n",
    "        else:\n",
    "            return partial(vPow,(self, Variable(b)))\n",
    "    \n",
    "    def __rpow__(self, b): #called if right operand is of this class\n",
    "        return partial(vPow,(self, Variable(b)))\n",
    "    \n",
    "    def __neg__(self):\n",
    "        return partial(vNeg,(self))\n",
    "        \n",
    "    def __str__(self):\n",
    "        return 'Value: {self.value}, Gradient: {self.grad}'.format(self=self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'functools.partial' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [33], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m c \u001b[38;5;241m=\u001b[39m Variable(\u001b[38;5;241m8\u001b[39m)\n\u001b[1;32m      4\u001b[0m e \u001b[38;5;241m=\u001b[39m Variable(\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m a \u001b[38;5;241m-\u001b[39m b \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m      6\u001b[0m res\u001b[38;5;241m.\u001b[39mbackProp()\n\u001b[1;32m      7\u001b[0m recursive_print_grads(res)\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'functools.partial' and 'int'"
     ]
    }
   ],
   "source": [
    "a = Variable(4)   # a = 4\n",
    "b = Variable(3)   # b = 3\n",
    "c = Variable(8)\n",
    "e = Variable(2)\n",
    "res = 1 + a - b - 2\n",
    "res.backProp()\n",
    "recursive_print_grads(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'functools.partial' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [56], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m res \u001b[38;5;241m=\u001b[39m vExp(((a \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m6\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mc)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m ) \u001b[38;5;241m/\u001b[39m (c\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m3\u001b[39m)))\u001b[38;5;66;03m# res = a * c = 28\u001b[39;00m\n\u001b[1;32m      2\u001b[0m res\u001b[38;5;241m.\u001b[39mbackProp()\n\u001b[1;32m      3\u001b[0m recursive_print_grads(res)\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'functools.partial' and 'float'"
     ]
    }
   ],
   "source": [
    "#res = vExp(((a - 4 + (6**c)) * -0.5 ) / (c**(3)))# res = a * c = 28\n",
    "res.backProp()\n",
    "recursive_print_grads(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### <font color=blue>Excercise 5 (Easy):</font>\n",
    "    \n",
    "<font color=blue>Write some code to manually check that your gradient calculation is correct, using the property of:</font>\n",
    "\n",
    "$$\n",
    "f'(x) = \\frac {f(x+\\epsilon) - f(x-\\epsilon)}{2 \\epsilon}\n",
    "$$\n",
    "\n",
    "<font color=blue>where $\\epsilon$ is a very small number to approximately calculate the gradient. Then use it to calculate the derivative of the function $f(x) = 21 * x^3$ at $x=3.2$. Double check that our framework gives you the same result.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fofx(x):\n",
    "    return 21 * (x**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test gradient:  645.3300000000007\n",
      "Framework grad:  645.1200000000001\n"
     ]
    }
   ],
   "source": [
    "x = Variable(3.2)\n",
    "res = fofx(x)\n",
    "res.backProp()\n",
    "def check_grad(res:Variable,e):\n",
    "    fprime = (fofx(x+e)-fofx(x-e)) / (2*e)\n",
    "    lambd = x.grad - fprime\n",
    "    print(\"Test gradient: \",fprime.value)\n",
    "    print(\"Framework grad: \",x.grad)\n",
    "\n",
    "check_grad(res,0.1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('chaos')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "c7ae596a0e351a64bdee8401d4598b089ee21c4ae0cc6b869fec54b3f2f67ab8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
